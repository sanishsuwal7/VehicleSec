{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pip Installs"
      ],
      "metadata": {
        "id": "8Y8QGIhq-k3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install cleverhans\n",
        "!pip install quantus\n",
        "!pip install captum\n",
        "!pip install ranger-adabelief"
      ],
      "metadata": {
        "id": "ETgLVNWO-oOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "EixHUPJz-srq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "import torch.optim as optim\n",
        "from cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent)\n",
        "\n",
        "import quantus\n",
        "import captum\n",
        "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "\n",
        "import os\n",
        "from itertools import chain\n",
        "\n",
        "from collections import Counter\n",
        "from ranger_adabelief import RangerAdaBelief"
      ],
      "metadata": {
        "id": "0QChV1hu-t6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isht6fyPAuQn"
      },
      "source": [
        "VGG 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r300c89xCZK"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        \"\"\"\n",
        "        Initialize the VGG model.\n",
        "\n",
        "        Args:\n",
        "            features: A sequential module representing the convolutional layers.\n",
        "        \"\"\"\n",
        "        super(VGG,self).__init__()\n",
        "        self.features = features\n",
        "\n",
        "        # Weight initialization for convolutional layers\n",
        "        # Iterate through all modules in the model\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                n = layer.kernel_size[0] * layer.kernel_size[1] * layer.out_channels\n",
        "                layer.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                # Bias initialization to zero\n",
        "                layer.bias.data.zero_()\n",
        "\n",
        "        # Fully connected layers for final classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(), # Dropout for regularization\n",
        "            nn.Linear(512,512), # Fully connected layer\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512,512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512,43)\n",
        "        )\n",
        "\n",
        "    # Function to create layers based on configuration\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the VGG model.\n",
        "\n",
        "        Args:\n",
        "            x : Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        inputs = self.features(inputs)\n",
        "        inputs = inputs.view(inputs.size(0), -1)\n",
        "        output = self.classifier(inputs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xg8rOWNrx9Q"
      },
      "outputs": [],
      "source": [
        "def make_layers(configuration):\n",
        "    \"\"\"\n",
        "    Function to create layers for VGG-16 based on configuration\n",
        "    :param configuration: configuration\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    in_channels =3\n",
        "\n",
        "    # Iterate through the configuration to build layers\n",
        "    for layer_config  in configuration:\n",
        "        # 'M' represents a Max Pooling layer\n",
        "        if layer_config  == 'Max':\n",
        "            layers += [nn.MaxPool2d(kernel_size = 2, stride =2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, layer_config , kernel_size = 3, padding =1) # Convolutional layer\n",
        "            layers.append(conv2d)\n",
        "            layers.append(nn.ReLU(inplace = True))\n",
        "            # Update the input channels for the next layer\n",
        "            in_channels = layer_config\n",
        "     # Return the sequential model of layers\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Two Conv layers (number of filters , number of filters) + Max Pool\n",
        "configuration = [\n",
        "    64, 64, 'Max',\n",
        "    128, 128, 'Max',\n",
        "    256, 256, 256, 'Max',\n",
        "    512, 512, 512, 'Max',\n",
        "    512, 512, 512, 'Max']\n",
        "\n",
        "def vgg16():\n",
        "    \"\"\"\n",
        "    Function to create a VGG-16 model\n",
        "    \"\"\"\n",
        "    return VGG(make_layers(configuration))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xoUgBTFA-P_"
      },
      "source": [
        "ResNet 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq_3GPcIbvq0"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"\n",
        "    3x3 convolution with padding.\n",
        "\n",
        "    Args:\n",
        "        in_planes : Number of input channels.\n",
        "        out_planes : Number of output channels (filters).\n",
        "        stride: Stride of the convolution. Default is 1.\n",
        "\n",
        "    Returns:\n",
        "        nn.Conv2d: A 2D convolutional layer with 3x3 kernel size.\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLk3UH-SFPMs"
      },
      "outputs": [],
      "source": [
        "def conv1x1(in_planes, out_planes, stride =1):\n",
        "    \"\"\"\n",
        "    1x1 convolution.\n",
        "\n",
        "    Args:\n",
        "        in_planes: Number of input channels.\n",
        "        out_planes: Number of output channels (filters).\n",
        "        stride: Stride of the convolution. Default is 1.\n",
        "\n",
        "    Returns:\n",
        "        nn.Conv2d: A 2D convolutional layer with 1x1 kernel size.\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride = stride, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkDWCgU2Ffsf"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1 # Expansion factor for output channels used in ResNet blocks\n",
        "    num_layers = 2  # Number of layers in the block\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        \"\"\"\n",
        "        Initialize a BasicBlock instance.\n",
        "\n",
        "        Args:\n",
        "            inplanes : Number of input channels.\n",
        "            planes : Number of output channels for the convolutions.\n",
        "            stride: Stride for the first convolution. Default is 1.\n",
        "            downsample : Downsampling layer to match dimensions of the input and output for the residual connection.\n",
        "        \"\"\"\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # only conv with possibly not 1 stride\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        # Batch Normalization\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        # ReLU activation\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # if stride is not 1 then self.downsample cannot be None\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the BasicBlock.\n",
        "\n",
        "        Args:\n",
        "            x : Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying the block.\n",
        "        \"\"\"\n",
        "        identity = x\n",
        "         # First convolution, batch normalization, and ReLU activation\n",
        "        output = self.conv1(x)\n",
        "        output = self.bn1(output)\n",
        "        output = self.relu(output)\n",
        "         # Second convolution and batch normalization\n",
        "        output = self.conv2(output)\n",
        "        output = self.bn2(output)\n",
        "\n",
        "        # Apply downsampling to the identity if needed\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # the residual connection\n",
        "\n",
        "        output += identity\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def block_conv_info(self):\n",
        "        \"\"\"\n",
        "        Retrieve information about the convolutional layers in the block.\n",
        "\n",
        "        Returns:\n",
        "            tuple: information about the convolutional layers\n",
        "        \"\"\"\n",
        "        block_kernel_sizes = [3, 3]\n",
        "        block_strides = [self.stride, 1]\n",
        "        block_paddings = [1, 1]\n",
        "\n",
        "        return block_kernel_sizes, block_strides, block_paddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_NLMfF-FnKd"
      },
      "outputs": [],
      "source": [
        "class ResNet_features(nn.Module):\n",
        "    '''\n",
        "    the convolutional layers of ResNet\n",
        "    the average pooling and final fully convolutional layer is removed\n",
        "    '''\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, filter=None, filter_layer=None):\n",
        "        \"\"\"\n",
        "        Initialize ResNet_features.\n",
        "\n",
        "        Args:\n",
        "            block: The residual block type\n",
        "            layers: Number of blocks in each layer\n",
        "            num_classes: Number of classes for classification.\n",
        "            zero_init_residual: Whether to zero-initialize residual batch norm weights.\n",
        "            filter: Custom filter.\n",
        "            filter_layer: Custom filter layer.\n",
        "        \"\"\"\n",
        "        super(ResNet_features, self).__init__()\n",
        "        self.inplanes = 64\n",
        "\n",
        "        # comes from the first conv and the following max pool\n",
        "        self.strides = [2, 2]\n",
        "        self.paddings = [3, 1]\n",
        "        self.kernel_sizes = [7, 3]\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Batch normalization\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # ReLU activation\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # Max pooling\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # Global average pooling\n",
        "        #self.global_pool = nn.AvgPool2d(kernel_size=7)\n",
        "\n",
        "        # Define the sequential layers using residual blocks\n",
        "        self.block = block\n",
        "        self.layers = layers\n",
        "\n",
        "\n",
        "\n",
        "        # Layer 1\n",
        "        self.layer1 = self._make_layer(block=block, planes=64, num_blocks=self.layers[0])\n",
        "        self.dn_layer1 = None\n",
        "\n",
        "        # Layer 2\n",
        "        self.layer2 = self._make_layer(block=block, planes=128, num_blocks=self.layers[1], stride=2)\n",
        "        self.dn_layer2 = None\n",
        "\n",
        "        # Layer 3\n",
        "        self.layer3 = self._make_layer(block=block, planes=256, num_blocks=self.layers[2], stride=2)\n",
        "        self.dn_layer3 = None\n",
        "\n",
        "        # Layer 4\n",
        "        self.layer4 = self._make_layer(block=block, planes=512, num_blocks=self.layers[3], stride=2)\n",
        "        self.dn_layer4 = None\n",
        "\n",
        "         # Final fully connected layer\n",
        "        self.fc = nn.Linear(512, 43)\n",
        "\n",
        "        # initialize the weights\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                nn.init.constant_(layer.weight, 1)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        if zero_init_residual:\n",
        "            for layer in self.modules():\n",
        "                nn.init.constant_(layer.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
        "        \"\"\"\n",
        "        Create a sequential layer consisting of multiple residual blocks.\n",
        "\n",
        "        Args:\n",
        "            block: The block type.\n",
        "            planes : Number of output channels for the blocks.\n",
        "            num_blocks: Number of blocks in the layer.\n",
        "            stride: Stride for the first block.\n",
        "\n",
        "        Returns:\n",
        "            nn.Sequential: A sequence of residual blocks.\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        # Downsample for dimension matching when stride > 1 or channel mismatch\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "          # First block with optional downsampling\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        #  Track kernel, stride, and padding info\n",
        "        for each_block in layers:\n",
        "            block_kernel_sizes, block_strides, block_paddings = each_block.block_conv_info()\n",
        "            self.kernel_sizes.extend(block_kernel_sizes)\n",
        "            self.strides.extend(block_strides)\n",
        "            self.paddings.extend(block_paddings)\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Standard forward pass through the network.\n",
        "        \"\"\"\n",
        "        inputs = self.conv1(inputs)\n",
        "        inputs = self.bn1(inputs)\n",
        "        inputs = self.relu(inputs)\n",
        "        inputs = self.maxpool(inputs)\n",
        "\n",
        "        inputs = self.layer1(inputs)\n",
        "\n",
        "        if self.dn_layer1 is not None:\n",
        "            inputs = self.dn_layer1(inputs)\n",
        "        inputs = self.layer2(inputs)\n",
        "        if self.dn_layer2 is not None:\n",
        "            inputs = self.dn_layer2(inputs)\n",
        "        inputs = self.layer3(inputs)\n",
        "        if self.dn_layer3 is not None:\n",
        "            inputs = self.dn_layer3(inputs)\n",
        "        inputs = self.layer4(inputs)\n",
        "        if self.dn_layer4 is not None:\n",
        "            inputs = self.dn_layer4(inputs)\n",
        "        inputs = F.adaptive_avg_pool2d(inputs, (1, 1))\n",
        "        inputs = inputs.reshape(-1, 512)\n",
        "\n",
        "        output = self.fc(inputs)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7rz8Gf0Frca"
      },
      "outputs": [],
      "source": [
        "def resnet_18(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    model = ResNet_features(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    return model"
      ]
    }
  ]
}