{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9797c5f7-c045-482b-98fb-a37c45e6a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import quantus\n",
    "import captum\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from lisa import LISA\n",
    "from itertools import chain\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9af08fb-40d2-45e0-b882-a0fa99fed2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0290a5e-3c3b-4aae-b83c-c1dc72536c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c7532a6-3ed3-4636-8229-0c00d67a17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.4563, 0.4076, 0.3895], std=[0.2298, 0.2144, 0.2259])\n",
    "\n",
    "# lisa_transforms = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)),transforms.ToTensor(),normalize])\n",
    "lisa_transforms = transforms.Compose([ transforms.ToPILImage(),transforms.ToTensor(),normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398ecabe-0d05-41a9-8446-3dd7d309c62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# # dataset =  LISA(root='./datasets' , download=True, train=True, transform = lisa_transforms)\n",
    "\n",
    "# train_dataset = LISA(root='./datasets', download=True, train=True, transform = lisa_transforms)\n",
    "# test_dataset = LISA(root='./datasets', download=True, train=False, transform = lisa_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3cea7d-7381-480e-90e5-bd253a4ca73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,) # num_workers=4,\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6478a801-b6c8-466c-98a5-bf0dc2e46cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ('stop', 'speedLimitUrdbl', 'speedLimit25', 'pedestrianCrossing', 'speedLimit35', 'turnLeft',\n",
    "#                       'slow', 'speedLimit15', 'speedLimit45', 'rightLaneMustTurn', 'signalAhead', 'keepRight',\n",
    "#                       'laneEnds', 'school', 'merge', 'addedLane', 'rampSpeedAdvisory40', 'rampSpeedAdvisory45',\n",
    "#                       'curveRight', 'speedLimit65', 'truckSpeedLimit55', 'thruMergeLeft', 'speedLimit30', 'stopAhead',\n",
    "#                       'yield', 'thruMergeRight', 'dip', 'schoolSpeedLimit25', 'thruTrafficMergeLeft', 'noRightTurn',\n",
    "#                       'rampSpeedAdvisory35', 'curveLeft', 'rampSpeedAdvisory20', 'noLeftTurn', 'zoneAhead25',\n",
    "#                       'zoneAhead45', 'doNotEnter', 'yieldAhead', 'roundabout', 'turnRight', 'speedLimit50',\n",
    "#                       'rampSpeedAdvisoryUrdbl', 'rampSpeedAdvisory50', 'speedLimit40', 'speedLimit55', 'doNotPass',\n",
    "#                       'intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9fd30bf8-8e77-4349-bde7-822c1da2d91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Get a sample image and label\n",
    "# sample_index = 6619\n",
    "\n",
    "# # Change index to see different samples\n",
    "# image, label = train_dataset[sample_index]\n",
    "\n",
    "# # Convert the image tensor to a format suitable for display\n",
    "# image = image.permute(1, 2, 0).numpy()  # Convert (C, H, W) to (H, W, C)\n",
    "# image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
    "\n",
    "# # Visualize the image\n",
    "# plt.imshow(image)\n",
    "# plt.title(f\"Label: {label} ({classes[label]})\")  # Display the label\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # 0 -stop  \n",
    "# # 3 -pedesterianCrossing  \n",
    "# # 5- turnleft\n",
    "# # 39-turn right\n",
    "# # 6-slow \n",
    "# # 10-signalahead \n",
    "# # 12-laneends \n",
    "# # 13-school \n",
    "# # 14-merge \n",
    "# # 24-yield \n",
    "# # 29-noturnright \n",
    "# # 33-noleftturn \n",
    "# # 38-roundabout\n",
    "\n",
    "# # 2 4 7 8 19 22 40 43 44 - speed limits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ff2f947-970f-4de7-bb14-cfce496daa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1dcc92e0-4965-458a-8f3b-9d9651da6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset creation complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the labels and their corresponding indexes\n",
    "classes = ('stop', 'speedLimitUrdbl', 'speedLimit25', 'pedestrianCrossing', 'speedLimit35', 'turnLeft',\n",
    "                      'slow', 'speedLimit15', 'speedLimit45', 'rightLaneMustTurn', 'signalAhead', 'keepRight',\n",
    "                      'laneEnds', 'school', 'merge', 'addedLane', 'rampSpeedAdvisory40', 'rampSpeedAdvisory45',\n",
    "                      'curveRight', 'speedLimit65', 'truckSpeedLimit55', 'thruMergeLeft', 'speedLimit30', 'stopAhead',\n",
    "                      'yield', 'thruMergeRight', 'dip', 'schoolSpeedLimit25', 'thruTrafficMergeLeft', 'noRightTurn',\n",
    "                      'rampSpeedAdvisory35', 'curveLeft', 'rampSpeedAdvisory20', 'noLeftTurn', 'zoneAhead25',\n",
    "                      'zoneAhead45', 'doNotEnter', 'yieldAhead', 'roundabout', 'turnRight', 'speedLimit50',\n",
    "                      'rampSpeedAdvisoryUrdbl', 'rampSpeedAdvisory50', 'speedLimit40', 'speedLimit55', 'doNotPass',\n",
    "                      'intersection')\n",
    "\n",
    "# Define the indexes for the subset\n",
    "included_indexes = {0, 3, 5, 6, 10, 12, 13, 14, 24, 29, 33, 38, 39}\n",
    "remapped_indexes = {2, 3, 7, 8, 19, 22, 40, 43, 44}\n",
    "\n",
    "# Paths to the image tensors and label tensor\n",
    "image_tensor_paths = [\n",
    "    \"datasets/lisa-batches/images_0.tensor\",  # Replace with the actual path\n",
    "    \"datasets/lisa-batches/images_1.tensor\",\n",
    "    \"datasets/lisa-batches/images_2.tensor\"\n",
    "]\n",
    "label_file_path = \"datasets/lisa-batches/labels.tensor\"  # Replace with the actual path\n",
    "\n",
    "# Load and concatenate all image tensors\n",
    "images_tensor = torch.cat([torch.load(path) for path in image_tensor_paths], dim=0)\n",
    "\n",
    "# Load the labels tensor\n",
    "labels_tensor = torch.load(label_file_path)\n",
    "\n",
    "# Initialize lists for the subset\n",
    "subset_images = []\n",
    "subset_labels = []\n",
    "\n",
    "# Filter and process the dataset\n",
    "for i, label in enumerate(labels_tensor.tolist()):\n",
    "    if label in included_indexes:\n",
    "        subset_images.append(images_tensor[i])\n",
    "        subset_labels.append(label)  # Keep the original label\n",
    "    elif label in remapped_indexes:\n",
    "        subset_images.append(images_tensor[i])\n",
    "        subset_labels.append(50)  # Remap the label to \"speedlimit\"\n",
    "\n",
    "\n",
    "# Convert the subset to tensors\n",
    "subset_images_tensor = torch.stack(subset_images)\n",
    "subset_labels_tensor = torch.tensor(subset_labels)\n",
    "\n",
    "# Save the new subset tensors\n",
    "torch.save(subset_images_tensor, \"datasets/lisa-batches/subset_images.tensor\")  # Replace with the desired save path\n",
    "torch.save(subset_labels_tensor, \"datasets/lisa-batches/subset_labels.tensor\")  # Replace with the desired save path\n",
    "\n",
    "print(\"Subset creation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "489e3a5a-c096-42af-93d2-01acd9728011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapped labels saved to: datasets/lisa-batches/subset_labels2.tensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to the original label tensor file\n",
    "label_tensor_path = \"datasets/lisa-batches/subset_labels.tensor\"  # Replace with the actual path\n",
    "\n",
    "# Load the label tensor\n",
    "labels_tensor = torch.load(label_tensor_path)\n",
    "\n",
    "# Define the mapping of original labels to new labels\n",
    "label_mapping = {\n",
    "    0: 0,  # Old 0 -> New 0\n",
    "    50: 1, # Old 50 -> New 1\n",
    "    3: 2,  # Old 3 -> New 2\n",
    "    5: 3,  # Old 5 -> New 3\n",
    "    6: 4,  # Old 6 -> New 4\n",
    "    10: 5, # Old 10 -> New 5\n",
    "    12: 6, # Old 12 -> New 6\n",
    "    13: 7, # Old 13 -> New 7\n",
    "    14: 8, # Old 14 -> New 8\n",
    "    24: 9, # Old 24 -> New 9\n",
    "    29: 10, # Old 29 -> New 10\n",
    "    33: 11, # Old 33 -> New 11\n",
    "    38: 12, # Old 38 -> New 12\n",
    "    39: 13  # Old 39 -> New 14\n",
    "}\n",
    "\n",
    "# Apply the label mapping to the tensor\n",
    "remapped_labels = torch.tensor([label_mapping[label.item()] if label.item() in label_mapping else -1 for label in labels_tensor])\n",
    "\n",
    "# Save the remapped labels tensor to a file\n",
    "save_path = \"datasets/lisa-batches/subset_labels2.tensor\"  # Replace with your desired save path\n",
    "torch.save(remapped_labels, save_path)\n",
    "\n",
    "print(f\"Remapped labels saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb45245-66e9-4e3a-9b3c-873e22c87569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in each label:\n",
      "Label 0: 1821 images\n",
      "Label 1: 838 images\n",
      "Label 2: 1085 images\n",
      "Label 3: 32 images\n",
      "Label 4: 34 images\n",
      "Label 5: 925 images\n",
      "Label 6: 210 images\n",
      "Label 7: 133 images\n",
      "Label 8: 266 images\n",
      "Label 9: 236 images\n",
      "Label 10: 26 images\n",
      "Label 11: 47 images\n",
      "Label 12: 53 images\n",
      "Label 13: 92 images\n",
      "Total number of images: 5798\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label_file_path = \"datasets/lisa-batches/subset_labels2.tensor\"  # Replace with the actual path\n",
    "\n",
    "# Load the labels tensor\n",
    "labels_tensor = torch.load(label_file_path)\n",
    "\n",
    "# Convert the tensor to a list for easy inspection\n",
    "labels_list = labels_tensor.tolist()\n",
    "\n",
    "label_counts = Counter(labels_list)\n",
    "\n",
    "# Print the unique labels and their counts\n",
    "print(\"Number of images in each label:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} images\")\n",
    "\n",
    "# Optional: Print the total number of images\n",
    "print(f\"Total number of images: {sum(label_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30435389-c61b-412a-8c0c-ae12df0febcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['stop','speedlimit', 'pedestrianCrossing', 'turnLeft','slow', 'signalAhead', 'laneEnds', 'school', 'merge','yield', 'noRightTurn','noLeftTurn', 'roundabout', 'turnRight']\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SubsetLISA(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset for the LISA subset, created using filtered image and label tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_tensor_path, label_tensor_path,train:bool, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tensor_path (str): Path to the images tensor file.\n",
    "            label_tensor_path (str): Path to the labels tensor file.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.images = torch.load(image_tensor_path)\n",
    "        self.labels = torch.load(label_tensor_path)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self._train_test_split()\n",
    "\n",
    "        assert len(self.images) == len(self.labels), \"Images and labels length mismatch\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label) where image is the input tensor and label is the target tensor.\n",
    "        \"\"\"\n",
    "        image = self.images[index]\n",
    "        target = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def _train_test_split(self, test_percent: float = 0.16):\n",
    "        classes = {}\n",
    "        for i, cl in enumerate(self.labels.numpy()):\n",
    "            arr = classes.get(cl, [])\n",
    "            arr.append(i)\n",
    "            classes[cl] = arr\n",
    "\n",
    "        train, test = [], []\n",
    "        for cl, arr in classes.items():\n",
    "            split_index = int(len(arr) * test_percent)\n",
    "            test = test + arr[:split_index]\n",
    "            train = train + arr[split_index:]\n",
    "\n",
    "        sub = train if self.train else test\n",
    "        self.images, self.labels = self.images[sub], self.labels[sub]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf3e7320-e064-4126-b6e6-ca49985c6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4876\n",
      "922\n"
     ]
    }
   ],
   "source": [
    "# Paths to the saved subset tensors\n",
    "image_tensor_path = \"datasets/lisa-batches/subset_images.tensor\"  # Replace with your actual file path\n",
    "label_tensor_path = \"datasets/lisa-batches/subset_labels2.tensor\"  # Replace with your actual file path\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = SubsetLISA(image_tensor_path, label_tensor_path, train=True, transform = lisa_transforms)\n",
    "test_dataset = SubsetLISA(image_tensor_path, label_tensor_path, train=False, transform = lisa_transforms)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ef10d4f-9b71-4416-b178-6da574499f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGZCAYAAABmNy2oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAj80lEQVR4nO3deXDddb3/8df3bDknOVmapUu6pCGlDS20IBQHQaX0ckcWhxEXBhwVxGVERsdR/5HBSkWrjncGcEBB2RxQGBBBvSpygat4KYqXq6UtSxeaLmmaNmtzspzt+/vD4fMjFtr3W1BkeD5m+genr7zzPevrfJucN1Ecx7EAAJCUeL0PAADwr4NSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAgFLAK7rtttsURZH+9Kc/vSbzoijS5Zdf/prMeunMr3zlK3/315dKJV111VVauHChampq1N3dre985zuuGWvXrtXSpUtVrVbNX7N582Z95Stf0Y4dO5xH7FcqldTV1aVrrrnmH/698MZHKeBN7bLLLtO6dev06U9/Wg8++KDe85736LOf/ay+/vWvm76+t7dX3/rWt7R27VolEvan0+bNm3XVVVf9U0ohnU7ry1/+stauXauBgYF/+PfDGxulgDetTZs26eabb9aaNWv0xS9+UaeffrrWrVunj33sY7r66qs1ODh4xBnXXnutmpqadP755/8Tjvjvd+GFFyqKIt14442v96HgXxylgFdlcnJSn//853X88cersbFRzc3NOuWUU/TAAw+84tfceOONWrx4sWpqarR06VLdddddh2T6+vr0yU9+UvPmzVMmk1FnZ6euuuoqlcvl1+zY77//fsVxrEsuuWTa5ZdccokmJib061//+rBfXywWdfPNN+uiiy465Czhu9/9rlasWKF8Pq/6+np1d3frS1/6kqS//rPc+9//fknSqlWrFEWRoijSbbfdFr7+lltu0YoVK5TNZtXc3Kz3vOc9euaZZ6Z9j4svvlj5fF6bNm3S6tWrVVdXp7a2Nl1++eUaHx+fls1kMrrgggt00003iR2YOBxKAa/K1NSUBgcH9YUvfEH333+/fvzjH+u0007T+eefrx/+8IeH5H/2s5/puuuu09q1a3Xvvfeqo6NDF154oe69996Q6evr08knn6wHH3xQX/7yl/WrX/1Kl156qdatW6ePf/zjRzymhQsXauHChUfMbdy4UW1tbZo9e/a0y5cvXx7+/nD+8Ic/aGBgQKtWrZp2+V133aXLLrtM73znO/XTn/5U999/vz73uc+pUChIks4555zwz1PXX3+91q9fr/Xr1+ucc86RJK1bt06XXnqpli1bpvvuu0/XXnutNmzYoFNOOUVbtmyZ9r1KpZLOPvtsrV69Wvfff78uv/xy3XjjjbrgggsOOd7TTz9dPT09R7xeeJOLgVdw6623xpLiJ5980vw15XI5LpVK8aWXXhqfcMIJ0/5OUpzL5eK+vr5p+e7u7njRokXhsk9+8pNxPp+Pe3p6pn39t7/97VhSvGnTpmkz16xZMy3X1dUVd3V1HfFYzzzzzHjJkiUv+3eZTCb+xCc+cdiv/+Y3vxlLmnZ94jiOL7/88ripqemwX3vPPffEkuJHH3102uVDQ0NxLpeLzz777GmX79y5M66pqYkvuuiicNlHPvKRWFJ87bXXTst+7WtfiyXFv//976ddvmXLllhS/N3vfvewx4Y3N84U8Krdc889OvXUU5XP55VKpZROp3XzzTcf8s8dkrR69WrNmjUr/HcymdQFF1ygrVu3avfu3ZKkX/ziF1q1apXa29tVLpfDn7POOkuS9Nvf/vawx7N161Zt3brVdOxRFP1dfyf99YfMURSptbV12uUnn3yyhoeHdeGFF+qBBx7QgQMHTMciSevXr9fExIQuvvjiaZfPnz9fZ5xxhh5++OFDvuaDH/zgtP++6KKLJEmPPvrotMtnzpwpSdqzZ4/5ePDmQyngVbnvvvv0gQ98QHPnztUdd9yh9evX68knn9RHP/pRTU5OHpL/23+qeellL/5mzL59+/Tzn/9c6XR62p9ly5ZJkutF9nBaWlpe9rdxCoWCisWimpubD/v1ExMTSqfTSiaT0y7/0Ic+pFtuuUU9PT1673vfq5kzZ+qtb32rHnrooSMe04vHM2fOnEP+rr29/ZDjTaVSamlpmXbZ396eL8pms+G4gVeSer0PAG9sd9xxhzo7O3X33XdPe2c9NTX1svm+vr5XvOzFF7fW1lYtX75cX/va1152Rnt7+6s9bEnScccdp7vuukt9fX3Tyurpp5+WJB177LGH/frW1lYVi0UVCgXV1dVN+7tLLrlEl1xyiQqFgn73u99pzZo1Ovfcc/X888+ro6PjFWe+eBvs3bv3kL/r7e095KykXC5rYGBgWjH87e35ohd/m+pvZwAvxZkCXpUoipTJZKYVQl9f3yv+9tHDDz+sffv2hf+uVCq6++671dXVpXnz5kmSzj33XG3cuFFdXV066aSTDvnzWpXCeeedpyiKdPvtt0+7/LbbblMul9O73vWuw359d3e3JGnbtm2vmKmrq9NZZ52lK664QsViUZs2bZIk1dTUSDr0Xfspp5yiXC6nO+64Y9rlu3fv1iOPPKLVq1cf8j3uvPPOaf/9ox/9SNJff7D8Utu3b5ckLV269LDXC29unCngiB555JGX/ZDV2WefrXPPPVf33XefLrvsMr3vfe/Trl279NWvflVz5sw55DdlpL++Sz3jjDN05ZVXqq6uTjfccIOeffbZab+WunbtWj300EN629veps985jNasmSJJicntWPHDv3yl7/U9773vVAgL2fRokWSdMSfKyxbtkyXXnqp1qxZo2QyqZUrV+o3v/mNbrrpJl199dVH/OejF190n3jiifAbS5L08Y9/XLlcTqeeeqrmzJmjvr4+rVu3To2NjVq5cqWk/38WctNNN6m+vl7ZbFadnZ1qaWnRlVdeqS996Uv68Ic/rAsvvFADAwO66qqrlM1mtWbNmmnHkMlk9B//8R8aGxvTypUr9fjjj+vqq6/WWWedpdNOO21a9oknnlAymdQ73vGOw14vvMm93j/pxr+uF3/76JX+vPDCC3Ecx/E3vvGNeOHChXFNTU18zDHHxN///vfjNWvWxH/78JIUf/rTn45vuOGGuKurK06n03F3d3d85513HvK99+/fH3/mM5+JOzs743Q6HTc3N8cnnnhifMUVV8RjY2PTZv7tbx91dHTEHR0dputYLBbjNWvWxAsWLIgzmUy8ePHi+LrrrjPfRm9/+9sP+U2h22+/PV61alU8a9asOJPJxO3t7fEHPvCBeMOGDdNy11xzTdzZ2Rknk8lYUnzrrbeGv/vBD34QL1++PM5kMnFjY2N83nnnTfutqzj+628f1dXVxRs2bIhPP/30OJfLxc3NzfGnPvWpabfRS4/13e9+t/m64c0pimM+yQL8vX7yk5/oggsuUE9Pj+bOnftP/d4XX3yx7r33Xo2NjR0xu23bNh199NF68MEHdeaZZ/4Tjg5vVPxMAXgVzj//fK1cuVLr1q17vQ/lsK6++mqtXr2aQsARUQrAqxBFkb7//e+rvb3dtSX1n6lcLqurq0vXX3/9630oeAPgn48AAAFnCgCAgFIAAASUAgAgMH947We3XOMaHGXtv57XfeLbXLPzsw7/oaKXKhZGXLOrsv+fqRIp3w8Wa9Kv/IGrQ2ZXi67ZiYT3/zNQMSfj4suvrHjFY4kcP6Yy/DrlS02NjpqzW57e5Jr97DPPuvILOrvM2eP+/d9ds7NN9lUUcTnjmp1I5e3hyPn51rQj6/xpZiXje4zHSfvzM4p8sxOqsc+uJo8ceqmSL+7Rkj1yhjMFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEJgXm+zftds1OO1Yr/L8H59yza44dqbs3b/NN9ux+6i93b6fRpJWdJ1qzqYi316lSmnSlS85dkJVpnyzaxKROZtw7r9J19iX63Q0zHDNznZ0uvKe/xPJ8J83umZPZuvt2XHfspz6xkZzdkbrfNfsdL39iV+KPYuSpP2Ffle+FE2Ys+msb/dR3nH/NMzwvU5kko7HrXM1leU8gDMFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAAC84ekB3bscA3e1bvJnC1MZlyzS2X7foFkzv5Rd0lauKjJnD165kmu2ak9veZs/x7fWpEtGze78vt39Ziz1SnfbZiO7GsuorJvnUdTY4M5m0wnXbOHR4Zd+YMHx83ZqXLFNbu/tsmcLUyOuWa3z7WvUVhxwttcs1vnLzZni+lm1+z1m33rcAqlA+ZsbUvRNXvenNnm7DFLjnfNnt263JytafCtClFbyxEjnCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAwLz7SCX7nhdJSlb2mbNtjUfex/FSs+csMGeP7j7GNXtex1xztq2+zjW7sHmHObv9iSdds//y+B9c+ZH+/eZsuurb25ORffdRIrbvsZKkXDZrzqYz3p1ak658YXLKPlu+67k3sh97KS67Zhd77LPzYyOu2dGKIXM22djhmj3w7LOu/NDkgDk7Y57vsZIcs1/PxJhvN9X++u3mbJS0v4RLUvtFHztihjMFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAAC82ekZ7XNcA1ecFSbOds22/dx97kLuszZllmLXLMTcdKcPfDMVtfsDY89bs5u2bDRNbs4MOjK5yv2tQvZhO+j9LVp+8qAXI1vvUCU+Me9j6k412LkHfnIObvWsb6gEtnXbUjS5JR9RcNIzzbX7L6sffVLc4fvvlyxxP68l6SJqN2crW3zPcYTSfvqimrRt+biwN7nzNnCmG8FjQVnCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACAwL/xYcYJvh1C+wb4rqa6u1TU7k2s2Z6uFSdfs0Z295uzzj//BNXvz//7FnJ0aGXHNnlFX68rn8/ZdPOkocs3OptLmbCKuumYnsvbZccK+30mSYpVc+Yrs88vyXc+aiv1YqlXf7PGqfc9Pacj3OCxs3WTO1iR8O5tmLn2rK988f6E5m22b5Zo9MWV/nRgv7/fNrthntzU2uGZbcKYAAAgoBQBAQCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBg/rx7c0uLa3AuZ19dkUjmXbPjibI5O7Z7r2v2lj//nzn77FN/dM0eHx01Z3MJX1/XOPOe1RXppH0tgiTXW41q7FtFkXCsrkiknO95IvvqD0lS1bHmouxboZFUxZytlH1rSJJV+6oQRb77Z3KsYM7u2bbVNXv/uCuurqL9dWJhrtE1u8GxXiJT9d33cXHQnM3V1LtmW3CmAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACCgFAAAAaUAAAgoBQBAQCkAAALzUpu4XPVNLtv3yMSVpGv02N4D5uy2p+y7jCRpy5/t+4yGel9wza7N1Jmz9RnHfhpJGefdk6zYd9pE8g0vV+z5cuzbC5Mo2vNRxfeeJxH5djzFjvdUcex7jKccxxIlfbMTjl1WcdL3OByvTpizwwfsz2NJ2jdadOXLjsd4tZp1zZ53zGxztmaG73GYd+xVSoz4dlOZZr7mEwEAb1iUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACAwf949V21zDY6KNebs1OA+1+ztTz1lzm78n8dcsw/s6jVnkyXfR8xzeftH6dPyrS6oVnzrIsanpszZSuRbc+HJxwnf7BrZ1y4kq77bsFwqu/KObR6qRs73X541F77JqnoeKlXfY3zC8ZyYsG/EkCSNVMdc+bGDW8zZ0RHf/bN/vMOc7XzLAtfsurx9RVB1atw124IzBQBAQCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABCYF6wkIucemaJ9wcpzGza6Zv/+4f82Z3u37nDNjkr2hTb5rH0PjyTVVOx7Skr21USSpMpk0ZWfnHAcS+zbrlNxvNeInTueatP2+yfl3Ks0MTXpykdJx3uqtH2XkSSVYns+aX8aS5IqZfuDK5Xy7YPK1Np3ntU22HeBSVJc9V3P4aL92Hu2bnXNHigPmLOFim/JU3dysTmbde7rsuBMAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAgFIAAATmZSLV7JBrcM9z9t0g//37x1yzn9++25zNphpds2vn2verJBpj1+yoYr8Nx4acs2vqXPls01xzdmbdbNfsRNV+LDXVvGt2bdq+h6kwut81e6To23/T0G6/nk3zZ7hml1Vvzh4c8+2/ObDvoDmbzvr2QS096RhztnnBTNfsyWLGlT84UDBnx0bsWUnasmWfORv12O9LSWrveqc529C5zTXbgjMFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAAC806H8di+XkCSnn/uKXO2t3ePa/bs2fPM2RXLT3bNnrus3ZyNaqZcs/c/+0dzdmTIt1aktbXNlZ/feYI52zxriWt2KtlkzlYHfWsUDu7pN2f37dzlmp1osK9okKTOExeasw1zfWtIShP29Sx92+wrFyRp7x77Y6sub1/7IkmLTz7WnM21+laclOV7DUom7es/Bvt7XLML//WgORtFo67Z4xn7a2cu4bt/LDhTAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAIF5cUZ5zLejZrRg38cyc7Zvb89bVpxmzi5f+U7X7MaFs8zZSlxwzR6d22LO7t35tGt2HPn2MNV21JuzDUfNcc1OZWaas8N/ec41e/8O+96eqNm3W2f+W45z5Ru7m83Zffu3uGZXR6rmbKp2tmt2Z/fR5mwy6XvfmEzaH+O9Owdcszdv3+jKz19kv3/qZxRds6t5+7E3Nde6Zjd3HTBna7K+56YFZwoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAgFIAAATmNReZZJNr8LxkuznbMc/3cffj5tSZs7nB2DV78ECfOTtSKrlmb99jz+/e6xqt3ft2uvKNR+82Z894l+96trcvMGf/67FHXLP//Ns/mrMnHLvCNfuopqNc+d7+MXP2J/f83DV75sS4OXvCiStdsxccvcScHRnyPTf37bFnt23rcc1+7I/Pu/InTdjX4Zyy6hTX7Fx6uTlbLdlXlkhSJe6wz65kXLMtOFMAAASUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACCgFAAAgXn3URSbo5J8Ozmmoqxr9mRdzpwdn7Lv+JGkjRt2mbObNvp2tzy/Z785O1L0LT8ajQ+48rOK9v1Rxx5vP25JamloMWeHhwZds3c7lkItWdTlmp1K+R7j48P2Y9+6abtvdsm+++jklSe5Zmcz9n1gg/FB1+yGllZz9piGxa7ZpWyzK995TLc529zc5pqdTdufP8PDvufm5HjFcRy+3W4WnCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABCYP9dfGZ5wDa5m7R8b/+OA/WPdkjQe2VdozFu2wzV7XjZvzmaqvo/GJ2P7x/S39E+6ZhfHhl15FewrHVIT9a7RDbJfz7Z6++0tSTVJezaV8K2tSMfOY0nYnxP1icg1Ox6zr35JOY9blao5Wq76Hoepevtzs7XJ9/x5x8wFrryS9ttwcNcLrtG7t2w1ZwtF36qQg8s7zdn6htf+fT1nCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACCwL4fJ+vbfzKidZ84e3P4n1+xnyqPmbGp8lmv28lnt5uzKf7NfR0ma3VEyZ+du9+3KefR/Blz5gv0mVGXY994hlWg0Z1vyM1yzc8m0OetYkyRJSpTtO4EkSSNFczQ3ZT9uSZpK2mdXan3XdDJjf2yVvLdibM+XR8Zco7dteNaV7+/tN2dHBoZds3ue/rM5O2Oeb8dTcWzInJ0s1bhmW3CmAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACCgFAAAAaUAAAgoBQBAQCkAAALz7qOots41uCZy7EoaHXfN3vL4NnN257NZ3+zZT5mzK49b5pq9dNFbzdm3NC9xzd72wmZXfkfvQXM2OeXbr5Ks1Jqz9dkG1+x00r6ua6ow4ZpdGpt05TNl+3uqlpxvx9POKfv9U0z6djZNxWVztprIuGarar9NDg6NuEY/9stfufI9z+82ZxPy3YYTyYo52zI775qdjO17r+LIua/LgDMFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAAC886AiXH7R+MlKZPbb87OrF/qml0YnW3OHhiyf9Rdkv63cMCcreafc80+utu+0qExt8A1e3ZLkys/0m9fLZKYHHXNjicGzdlM0rdGIZeyr9wYH7Q/BiVpbN9OV76x/ihz9tjuf3PNLhX+05ytbfetISmpZM5WJ+0rSyQprVnmbKEy5po9Opl25Yen7Ncznfa9P44z9nUrlWrSNTvjeKmtjx3rhIw4UwAABJQCACCgFAAAAaUAAAgoBQBAQCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAACBeYFHHEeuwXW1eXN25bGLXbOPX2rf9VKote8/kaQDJfs+o7rGKdfsZCZnzk5OTbhmV8qTrnzGcXemYtdoJWTf9VLf4NvbM6POviupeHDENXv/Lt/uo7aTuszZE09f7po9v6Zizs6Z3eSa3bt1hz087ttNlba/pChbY3+NkKTGJt+en6Zmez6RrLpmj1XsC4omSr69cZMV+xOu+g94X8+ZAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACCgFAAAgfkz6bm6BtfgONtqzrZn9rpm5zP2LqvtWOGaXVy01JyNW3wrNHKD+8zZwa2+lQsjg32uvCbtazGqY74VGsX+g+ZsKh53zZ41I23OFoYLrtn9u7b6jqVjpjnbvGiha/bcukZzdrjHd9z9z/zFnJ3aO8M1e1e11pyNU02u2aUx3/2ZTtifn1HK91yOZX8cTjrXxBSS9tUihapvPYcFZwoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgMO8+qsS+vSO9NaPm7NOlTa7ZSccunuNaJlyz58bd9uMYt+8okaQdz+wxZ5//82bX7IODI658pWTfOdS3c6Nr9s6Gijk7OTTgmm3frCNFmcg1u6/vGVe+9Cf7jqemvrmu2akx+2Orv2+3a/be3hfM2cKo+SVCkrRrd785m2xqcc0emjrgyo8npszZOEq6ZqvJvhMqN3++a3S5tc2cHZ6wX0crzhQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABJQCACCgFAAAAaUAAAjMn2EfHPJ9xHw4KpqzAyn72gpJ6nnGvnZh63NbXLPbnlxgzuZq8q7Z/S/sMmenJsZcs6M4duVztfZ8356trtmFQfs6j5qSc71A1f6x/lTSt+Zi6OCwKz+23X4fPb/N9zis7s6as8Wqb5XLVNn+3Kwm0q7ZyvaZo0O9O1yjK7X21SySFKft938q51mgIrUd1WnOdhy7zDU7yttfV3bv9q04seBMAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAgFIAAATm3Uejw1XX4FmJ5ebsipZm1+xS+nfm7DPbH3bN3vyCfa9Sc33ZNXveiD2bndHgmp2qz7nyStt3DqWqvvt+qmjfZZUs+nbOpJL2fBT7bpMZFfPTQZIU77Nfz9KEfd+QJPUnHflsxjU7mbTv1mlI+/Z71ci+b2hGXHLNnvI9DFXn2E1W3zLbNfv4pd3mbOsc3+wdO3rM2c2OrBVnCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABObP9Q+NH3QNrm+x903XCvtHxiUpU29fR5Bvs68ikKRntj1hD5cOuGbXOrZiJLP2NRSSFPniKlVi+2x7VJKUStnv+2TFNzuZtK9RyGSyvtnOG3HKk3euaGiut6/zqDrXXJQi+22Ysr9ESPLdP5Vq2jV70vlAbF+40JztWHqMa/acjvnm7IT9JpEkDY7YX2sLExO+4QacKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAIDAvNhktOTbIbRnZLc52z5rnmt22+I55uyJjae4ZmfnFc3Z3u1/cc2u3WrflTSlKdfsctGxWElSUvaFLHV53w6hmrR9X07Gt/5G5Sn7/pu46lw4lPIdTOTIxynnLqvYnk8nfO/tyrH9MV51ZCUpiu23SSbre1zl6vKufPvRR5uzbV0drtmjVfvtsndwyDV7YGzcnK0knEvPDDhTAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAIF5SU1J9n0ckrT/4E5zdirh29vT0txqzs446ijX7EX19usZRwXX7GhgvzlbKfh2zjTVNbryc5rn2rPz2l2zEyn7XqWR/r2u2f27BszZyULFNbtczbnylaR9d081U3LNTpTst2GqYs9KUkL259t42bfzLBXVmbNNs2a7ZjctWuDKNy9eZM4eTNt3aknS3r4+c7Z//7Br9pjjuR9XfcdtwZkCACCgFAAAAaUAAAgoBQBAQCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAACBec1FOZ5yDY5j+0fp+4f2uWZPxfb1BakG30fp61pnmrNHLV3qml0cecGcLez33d75dJMrf1S7fWXArMVdrtllOe6fhO96jg7Z15CUSr7Z5UrSl6/asxMV38oNOdZcjI0cdI0eq4yZs8Wkb43CjHyDOds4u801u/3Yblc+1ZA3Z3sP7HHNHhwbMWcnK84VJ0n747A2W+uabfr+r/lEAMAbFqUAAAgoBQBAQCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBg3n00UZlwDU4m7H1TVNo1u1y273oZ73MsqJEUlYbN2XyUdc3uOPEY+3Hs893eU72+/TejIwfs2Q32fUOSNDoxac7u69/hmj3cb985Exd973mSCd8unoNF+26loYLv/iyX7LuSDk75dutUM/a9Sg2z61yzGzrmmbOt3fb9W5Kket/zbc9gvznbPzLomu1ak+V7eVMisg+vT9f4hlu+/2s+EQDwhkUpAAACSgEAEFAKAICAUgAABJQCACCgFAAAAaUAAAgoBQBAQCkAAALzmotK0hyVJCWTjr6J7R+7l6SJon3twtiAb0XD4N4ex4EMuGa3HJszZ2fNbnTNTlV8n6Xf/pft5uzW7X2u2cMF+/qHTK3vfUmqGpuzjbm8a3Zt3rdGIRVlzNlMyTe7Ivv1nJm2H4ckpZpqzdk5R891zV6wpNOczTc3uWZvH9rvyu/du9ecnSj7VoUkE/bbvDhpX1kiSZWkfc1FNnrt39dzpgAACCgFAEBAKQAAAkoBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAAC80KjKGvf2yNJcWTf3aKKb/dRtWTfUxI79ohI0tDBMXN2z5ZtrtkN1Tpz9vj2Ntfsjlbfnp+O4xeZs9l5C1yzS1X7HqZ0zrcXJl2x5xty9h0/kpTPzXHlE9kac9Z3LaVErf1xm5k10ze7tdmczbXVu2anM/bn8mDvLtfs0T3PufJjBw+Ys5Wy73Uiln2XVanqGq1Kzn4skxOTvuEGnCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAAkoBABCY11yUk741F9Wq/bPdCfthSJLqshlztiLfCo2oas+PDE+4Zv/fU0PmbLbs6+v8sb51BB0rlpiz85zrHzK19rULccV3G5Y9qwsmRl2zo7J9PYckRbH9cVgt+e7Pmpn2FR01He2u2ZrjWKGS9j03p0bs98/EpH2ljCTFpXFXviZhXy5Scb4/rtg37aga+VZoOF6CND5UcM224EwBABBQCgCAgFIAAASUAgAgoBQAAAGlAAAIKAUAQEApAAACSgEAEFAKAICAUgAABFEcx/HrfRAAgH8NnCkAAAJKAQAQUAoAgIBSAAAElAIAIKAUAAABpQAACCgFAEBAKQAAgv8Hcux4z6DqAXgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a sample 4796 and label\n",
    "sample_index = 800\n",
    "\n",
    "# Change index to see different samples\n",
    "image, label = train_dataset[sample_index]\n",
    "\n",
    "# Convert the image tensor to a format suitable for display\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "print(image.shape)# Convert (C, H, W) to (H, W, C)\n",
    "image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Label: {label} ({classes[label]})\")  # Display the label\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d471f80-9384-4686-afd2-cb83ec6bb578",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,) \u001b[38;5;66;03m# num_workers=4,\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_dataloader[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,) # num_workers=4,\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(test_dataloader[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e9f8d8a6-2244-4f86-b325-eab097995dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c7d9f42e-b0be-419c-8034-12b71424c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate model!\n",
    "        if epochs%10==0:\n",
    "            predictions, labels = evaluate_model(model, test_dataloader, device)\n",
    "            test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - test accuracy: {(100 * test_acc):.2f}% and CE loss {loss.item():.2f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f0e245b3-5b94-40b6-a86d-9a09027f379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - test accuracy: 19.85% and CE loss 2.02\n",
      "Epoch 2/100 - test accuracy: 31.56% and CE loss 1.88\n",
      "Epoch 3/100 - test accuracy: 31.56% and CE loss 2.06\n",
      "Epoch 4/100 - test accuracy: 26.68% and CE loss 0.98\n",
      "Epoch 5/100 - test accuracy: 38.94% and CE loss 1.21\n",
      "Epoch 6/100 - test accuracy: 35.03% and CE loss 2.05\n",
      "Epoch 7/100 - test accuracy: 23.10% and CE loss 1.56\n",
      "Epoch 8/100 - test accuracy: 61.82% and CE loss 1.15\n",
      "Epoch 9/100 - test accuracy: 70.93% and CE loss 0.83\n",
      "Epoch 10/100 - test accuracy: 76.25% and CE loss 0.06\n",
      "Epoch 11/100 - test accuracy: 77.77% and CE loss 0.78\n",
      "Epoch 12/100 - test accuracy: 84.27% and CE loss 0.13\n",
      "Epoch 13/100 - test accuracy: 85.25% and CE loss 0.36\n",
      "Epoch 14/100 - test accuracy: 85.25% and CE loss 0.09\n",
      "Epoch 15/100 - test accuracy: 89.37% and CE loss 0.06\n",
      "Epoch 16/100 - test accuracy: 93.17% and CE loss 0.01\n",
      "Epoch 17/100 - test accuracy: 92.62% and CE loss 0.04\n",
      "Epoch 18/100 - test accuracy: 94.69% and CE loss 0.00\n",
      "Epoch 19/100 - test accuracy: 92.41% and CE loss 0.01\n",
      "Epoch 20/100 - test accuracy: 93.93% and CE loss 0.00\n",
      "Epoch 21/100 - test accuracy: 92.84% and CE loss 0.00\n",
      "Epoch 22/100 - test accuracy: 92.08% and CE loss 0.00\n",
      "Epoch 23/100 - test accuracy: 95.66% and CE loss 0.00\n",
      "Epoch 24/100 - test accuracy: 96.31% and CE loss 0.00\n",
      "Epoch 25/100 - test accuracy: 95.12% and CE loss 0.00\n",
      "Epoch 26/100 - test accuracy: 96.31% and CE loss 0.00\n",
      "Epoch 27/100 - test accuracy: 96.31% and CE loss 0.00\n",
      "Epoch 28/100 - test accuracy: 96.20% and CE loss 0.00\n",
      "Epoch 29/100 - test accuracy: 96.20% and CE loss 0.00\n",
      "Epoch 30/100 - test accuracy: 96.20% and CE loss 0.00\n",
      "Epoch 31/100 - test accuracy: 96.20% and CE loss 0.00\n",
      "Epoch 32/100 - test accuracy: 96.20% and CE loss 0.00\n",
      "Epoch 33/100 - test accuracy: 96.31% and CE loss 0.00\n",
      "Epoch 34/100 - test accuracy: 96.42% and CE loss 0.00\n",
      "Epoch 35/100 - test accuracy: 96.42% and CE loss 0.00\n",
      "Epoch 36/100 - test accuracy: 96.42% and CE loss 0.00\n",
      "Epoch 37/100 - test accuracy: 96.53% and CE loss 0.00\n",
      "Epoch 38/100 - test accuracy: 96.42% and CE loss 0.00\n",
      "Epoch 39/100 - test accuracy: 96.53% and CE loss 0.00\n",
      "Epoch 40/100 - test accuracy: 96.53% and CE loss 0.00\n",
      "Epoch 41/100 - test accuracy: 96.64% and CE loss 0.00\n",
      "Epoch 42/100 - test accuracy: 96.53% and CE loss 0.00\n",
      "Epoch 43/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 44/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 45/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 46/100 - test accuracy: 96.64% and CE loss 0.00\n",
      "Epoch 47/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 48/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 49/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 50/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 51/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 52/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 53/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 54/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 55/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 56/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 57/100 - test accuracy: 96.53% and CE loss 0.00\n",
      "Epoch 58/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 59/100 - test accuracy: 96.10% and CE loss 0.00\n",
      "Epoch 60/100 - test accuracy: 96.64% and CE loss 0.00\n",
      "Epoch 61/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 62/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 63/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 64/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 65/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 66/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 67/100 - test accuracy: 96.85% and CE loss 0.00\n",
      "Epoch 68/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 69/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 70/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 71/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 72/100 - test accuracy: 96.75% and CE loss 0.00\n",
      "Epoch 73/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 74/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 75/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 76/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 77/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 78/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 79/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 80/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 81/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 82/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 83/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 84/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 85/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 86/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 87/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 88/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 89/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 90/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 91/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 92/100 - test accuracy: 97.07% and CE loss 0.00\n",
      "Epoch 93/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 94/100 - test accuracy: 97.29% and CE loss 0.00\n",
      "Epoch 95/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 96/100 - test accuracy: 97.18% and CE loss 0.00\n",
      "Epoch 97/100 - test accuracy: 97.40% and CE loss 0.00\n",
      "Epoch 98/100 - test accuracy: 96.96% and CE loss 0.00\n",
      "Epoch 99/100 - test accuracy: 97.29% and CE loss 0.00\n",
      "Epoch 100/100 - test accuracy: 97.07% and CE loss 0.00\n"
     ]
    }
   ],
   "source": [
    "model_normal = train_model(model = model.to(device), epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "31661518-2fc0-41d0-8b1d-06877e58b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test accuracy: 97.07%\n"
     ]
    }
   ],
   "source": [
    "# Model to GPU and eval mode.\n",
    "model_normal.to(device)\n",
    "model_normal.eval()\n",
    "\n",
    "# Check test set performance.\n",
    "predictions, labels = evaluate_model(model_normal, test_dataloader, device)\n",
    "test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())        \n",
    "print(f\"Model test accuracy: {(100 * test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c2a563b0-6a17-4665-9410-8e5c788d168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model: models\\lisa_normal_subset.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path(\"models\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_name = \"lisa_normal_subset.pth\"\n",
    "model_save_path = model_path / model_name\n",
    "\n",
    "print(f\"Saving the model: {model_save_path}\")\n",
    "torch.save(obj=model_normal.state_dict(), f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "18a6056e-bc6a-487f-bf9b-4c22c6c79756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = vgg16().to(device)\n",
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e3263d9a-784e-4f9b-9660-c8560275c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for VGG LISA Normal is: 97.07%\n"
     ]
    }
   ],
   "source": [
    "# Model to GPU and eval mode.\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Check test set performance.\n",
    "predictions, labels = evaluate_model(model, test_dataloader, device)\n",
    "test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())        \n",
    "print(f\"Test accuracy for VGG LISA Normal is: {(100 * test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45a931-fc75-4baf-b930-8d567eb10a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb76b9-954f-40a7-accb-e0c882e56bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
